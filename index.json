[{"categories":[],"content":"I‚Äôm Dipam, a technology enthusiast, science nerd, and lover of food and travel. I‚Äôm currently working as a machine learning engineer at AIcrowd Research. Primarily, I work on hosting research oriented competitions, especially focussed on reinforcement learning. AIcrowd has hosted many research competitions in machine and reinforcement learning. In fact, I joined AIcrowd after winning 3rd position in the NeurIPS 2020 Procgen competition hosted there. I‚Äôve contributed heavily to the NeurIPS 2022 Citylearn Challenge, Multi Agent Behavior Challenge 2022, ZEW Data Purchasing Challenge, NeurIPS 2021 Deepracer Challenge, Flatland 3, and Multi Agent Behavior Challenge 2021. Previously, I was at Intel, where I worked on fault detection of industrial parts. The final solution comprised on a combination of detection using supervised learning with CNNs, and prediction via unsupervised learning using autoencoders. Prior to that, I also worked on developing hardware acceralators for CNNs, based on SystemVerilog and OpenCL. I love participating in competitions, although I have not found the time to do so recently. On Kaggle, I‚Äôve participated in a bunch of competitions for Computer Vision, and won some silver medals. I also took part in NeurIPS 2020 Procgen Competition, which highly motivated to pursue reinforcement learning research in the future. I‚Äôm fascinated with the current pace of progress in AI, and yet it feels like there are more questions than ever. Deep reinforcement learning in particular, feels extremely promising to me, and one of the open questions around it how to make Deep RL agents generalize. My undergraduate and masters studies are in Electronics and Communcation Engineering, at NIT Rourkela, India. There, I worked hands on in robotics, primarily with embedded systems, vision and control. I was one of the first members of Team Tiburon where we developed an autonomous underwater vehicle from scratch. It is certainly one of the most challenging projects I had worked on. I learned immensely about not just robotics and product development, but also teamwork and leadership. For the team, participating in competitions was a strong motivator, and we won 2nd position in the Singapore AUV Competition (SAUVC) 2018, becoming the first team from India to win a prize at SAUVC. I love to travel, especially places where I can go trekking or diving. Pictures speak for themselves. I‚Äôm a huge fan of many science fiction and fantasy series. Especially the Star wars and Tolkein universes, clich√©, I know; but they‚Äôre so popular for a reason. üòâ Have a research idea you‚Äôd like collaborate on around machine learning or reinforcement learning? Or just want to nerd out with me about your favourite fictional universe, discuss technology, science, politics and philosophy? Feel free to reach out to me on Twitter or email. üòÉ ","date":"2022-02-10","objectID":"/about/:0:0","tags":[],"title":"About me","uri":"/about/"},{"categories":["Machine Learning"],"content":"Notes on the ARC benchmark, my experience at the competition, and how it can become a long-term benchmark in AI.","date":"2022-02-07","objectID":"/thoughts-on-the-arc-benchmark/","tags":["Deep Learning","Benchmarks"],"title":"Thoughts on the ARC Benchmark","uri":"/thoughts-on-the-arc-benchmark/"},{"categories":["Machine Learning"],"content":"Notes on the ARC benchmark, my experience at the competition, and how it can become a long-term benchmark in AI - Twitter Thread The ARC Benchmark Around the end of 2019, Fran√ßois Chollet released the Abstraction and Reasoning Corpus (ARC) benchmark, along with the paper On the Measure of Intelligence. And in the paper, he made a powerful claim: that deep learning cannot solve the ARC benchmark. On the surface, the tasks in the benchmark are simple. You get a few example images for a task that applies some simple logic to the image. And then you‚Äôre asked to generate images for one or more test images that follow the same logic. Pretty much like a question format used in common IQ tests. Here are some examples: Two tasks from the train set. My descriptions of the tasks: Left ‚ÄòMake the structure symmetrical, add the added part red‚Äô, Right: ‚ÄòFill the smallest box with magenta, the next with orange, and the largest with sky blue.‚Äô The paper makes the claim that Deep Learning is ‚Äúconceptually similar to a locality-sensitive hashtable‚Äù, and cannot generalize beyond the kind of data it is trained on. This was pretty new to me, I was a noob in the ML space, but it really piqued my interest. The paper describes in detail the kinds of generalizations, and how to measure them; and why deep learning cannot generalize to these higher forms of generalization (such as what will be required to solve ARC). I highly recommend going through the paper, in this blog I‚Äôll only focus on the ARC benchmark. What concepts does the benchmark use to generate the tasks? - The goal is to encode priors that are common to humans, that‚Äôs no simple goal, and is vaguely understood. To quote the paper ‚ÄúWhat is the exact list of knowledge priors that humans are born with? This is the question that the developmental science theory of Core Knowledge seeks to answer.‚Äù These are the broad categories of priors that are used Objectness and elementary physics - A red circle on a black background, and a line at the edge, we imagine a sphere hitting a wall. Agentness and goal-directedness - A blue dot in an orange maze, our immediate intuition is the blue dot is an agent. Natural numbers and elementary arithmetic - Counting, adding, subtracting, we naturally do it without being taught. Elementary geometry and topology - Rotating a square by 45 degrees makes a diamond, we‚Äôre easily able to visualize this. ","date":"2022-02-07","objectID":"/thoughts-on-the-arc-benchmark/:0:0","tags":["Deep Learning","Benchmarks"],"title":"Thoughts on the ARC Benchmark","uri":"/thoughts-on-the-arc-benchmark/"},{"categories":["Machine Learning"],"content":"Why can‚Äôt deep learning solve ARC? Chollet‚Äôs argument is that deep learning is just pattern matching on low dimensional data that is its training dataset. But wait, aren‚Äôt images ‚Äúhigh dimensional‚Äù, and you can‚Äôt really interpolate high dimensional data? Yann LeCun certainly says so. So who is right? Well, they‚Äôre kind of talking about different things. Yes, images are high dimensional, if you consider the individual pixels as independent variables. But for a dataset that represents a concept, e.g ‚Äúnatural images‚Äù, pixels are not independent. Its pretty intuitive to understand that pixel values near each other on the same object will be close in value. But an extended idea on this is that there exists a low dimensional manifold in the high dimensional euclidian space that encompasses the entire dataset. This is known as the manifold hypothesis, here‚Äôs my favourite video that explains the manifold hypothesis. Note that this doesn‚Äôt make the manifold hypothesis necessarily correct, for an extended discussion check out this video from the Machine Learning Street Talk podcast. This debate is long ongoing, here‚Äôs a twitter query to put you down a rabbit hole. üòâ The point is, to solve the tasks on the ARC Benchmark, deep learning cannot leverage pattern matching. The tasks are hand designed and independent, not generated by some underlying code. In a later interview, Chollet shares that coming up with these tasks is hard and takes a lot of time. The benchmark is limited to 300 tasks total. 100 for ‚Äútraining‚Äù, 100 for ‚Äútesting‚Äù, and 100 secret tasks for evaluation. There‚Äôs no need follow the split as such, though the training tasks are somewhat easier. ","date":"2022-02-07","objectID":"/thoughts-on-the-arc-benchmark/:0:1","tags":["Deep Learning","Benchmarks"],"title":"Thoughts on the ARC Benchmark","uri":"/thoughts-on-the-arc-benchmark/"},{"categories":["Machine Learning"],"content":"Challenge Design The competition was designed as many others on Kaggle. You get a dataset to train on, and you‚Äôre expected to submit code that works on the evaluation dataset, without ever seeing the evaluation data. Obviously, there‚Äôs a time limit to run your code. Of course, ARC is not a standard machine learning benchmark. There are only 200 tasks, the only thing common between them is the human like priors described in the above section. How does one approach such a competition? - ARC is pretty different from most other competitions. While IQ test benchmarks are not uncommon, most formulate it as a classification problem. ARC requires you to generate the exact answer, which in turn means you need not only need a fuzzy understanding of concepts (something one might hope Deep learning can do, though manifold hypothesis suggests it can‚Äôt); and also have a module that produces exact results. For the latter, an AI that generates programs is a good approach. So you would need to combine fuzzy heuristics which does the ‚Äúabstraction and reasoning‚Äù, with a program generation process. This makes the challenge exceedingly difficult in my opinion, but that‚Äôs what makes it fun! Here‚Äôs Chollet‚Äôs recommendation on how to work on the competition. ","date":"2022-02-07","objectID":"/thoughts-on-the-arc-benchmark/:0:2","tags":["Deep Learning","Benchmarks"],"title":"Thoughts on the ARC Benchmark","uri":"/thoughts-on-the-arc-benchmark/"},{"categories":["Machine Learning"],"content":"One major flaw The challenge had one major flaw in my opinion. There was no private leaderboard. All the 100 evaluation tasks were on the public leaderboard. Organizers claimed that they‚Äôre confident no one can beat even 20 of the 100 tasks (Spoiler: the top solution beat 21). A lot of great engineering ideas were implemented, and the shared solutions are a goldmine of ideas. Nevertheless, not having a private leaderboard meant overfitting to the public leaderboard. It becomes a guessing game of what the tasks could be, and incorporate code to run that idea. Even if you don‚Äôt explicitly probe the tasks, say you add some code and it doesn‚Äôt move the scores, you‚Äôre likely to remove it (remember there‚Äôs the time limit to run your code). This defeats the purpose of generalization, in the sense that no new major solutions were found, just a lot of engineering to incorporate priors and do fast program search. ","date":"2022-02-07","objectID":"/thoughts-on-the-arc-benchmark/:0:3","tags":["Deep Learning","Benchmarks"],"title":"Thoughts on the ARC Benchmark","uri":"/thoughts-on-the-arc-benchmark/"},{"categories":["Machine Learning"],"content":"Ideas shared by participants As the competition progressed, participants shared a wide variety of ideas they were looking into. Decision trees, genetic algorithms, vision models, sequence to sequence models, and cellular automata to name a few. None of them could be used out of the box per se, the priors that the tasks used had to be hand-coded somehow. One thing was common, participants often created their own domain-specific language (DSL) for the tasks, based on the priors. For example, one had to find ‚Äúobjects‚Äù in the images, possibly downscale/upscale them, rotate them, etc. See the below figure for an example task. All of these were specific to the ARC benchmark, and everyone set out to make high quality implementations of these priors. Humans would likely think of this as a two ‚Äòstep‚Äô task: 1. Cut out the object 2. Repeat it horizontally, one time. Even to my newbie‚Äôs brain, there seemed to be some critical problems with all these approaches. First, the DSLs were catered for the ARC dataset, not really generalized, they reminded me of feature engineering for a specific task/dataset. In that sense, a 3 month competition is too short for such a benchmark, where you need to design your DSL, try different algorithms to use it, and make it efficient to meet the compute limits, while each step depends on the others. Second, it wasn‚Äôt clear that using a DSL was even the right approach. Yes, Chollet had suggested it so everyone was trying to make one (myself included), but if we‚Äôre trying to generate programs that solve for general tasks, would a DSL really be the right approach? When humans approach the problems, the priors are used, but often we come up with some one-off operation that solves the tasks combined with common functions in the priors (see example below). I suppose one could incorporate these one-off operations into the DSL. But keep adding too many and you‚Äôd probably have been better off using a general language rather than DSL. My description of the steps: 1. Select blue objects 2. Add orange squares to the NESW squares of the blue objects 3. Select red objects 4. Add yellow squares to the diagonal squares of the red objects. To me steps 2 and 4 feel really ‚Äòone-off‚Äô, not quite something I‚Äôd include in a DSL. Sure we can break it down further to fit some other general DSL, but nothing super simple. ","date":"2022-02-07","objectID":"/thoughts-on-the-arc-benchmark/:0:4","tags":["Deep Learning","Benchmarks"],"title":"Thoughts on the ARC Benchmark","uri":"/thoughts-on-the-arc-benchmark/"},{"categories":["Machine Learning"],"content":"Top solutions TL;DR - Everyone had a bunch of hand-crafted DSLs, and optimized code for fast search. The amount of effort the top participants put is truly awe-inspiring! The only thing that slightly disappointed me was the number of DSLs that have so much in common, but were designed by everyone independently, and with custom fast software. In a sense, the competition strongly incentivized this due to the compute limit. Some solutions used hand crafted heuristics to filter their search process. Some hand coded extra tasks to improve their DSL evaluation. The top DSLs had these common themes: Cut parts from input by color/shape/location/etc Recolor parts Check symmetries Moving and replicating objects And many others that I‚Äôm leaving out for brevity. Interestingly, nearly everyone converged to the same ‚Äúhuman like operations‚Äù. It begs the question, is that the correct approach to solving ARC, and Abstraction and Reasoning as a problem in general? First place solution - Worth the read if you‚Äôre interested. Collection of all top solutions Interestingly, and perhaps obvious in hindsight, GPT-3, which came out later that year, completely failed the ARC tasks. ","date":"2022-02-07","objectID":"/thoughts-on-the-arc-benchmark/:0:5","tags":["Deep Learning","Benchmarks"],"title":"Thoughts on the ARC Benchmark","uri":"/thoughts-on-the-arc-benchmark/"},{"categories":["Machine Learning"],"content":"Ideas I tried DSL and Tree search - Like most others, I also tried to create a DSL that catered to the ARC tasks. I wouldn‚Äôt say it was anything out of the ordinary, I had organized it in a way that functions operate on scalars, images, list of scalars, and list of images. For example, one function would take an image and split it into multiple images based on color, another would find the area of each object in the image, and so on. I performed simple Depth First Search on this DSL, discarding nodes based on some heuristics like absence of different colors in the outputs. You can check the full code here. The code solved 20 training tasks, 8 test tasks, and only 1 evaluation task.üòÇ Here‚Äôs and example DSL function: @register(\"Image\", \"ImageList\") def split_by_color(img, crop=True): \"\"\" Input an image, splits image list of images by different colors\"\"\" if img.max() == 0: return None cols = np.unique(img[img \u003e 0]) outs = [] for c in cols: colimg = np.zeros_like(img) colimg[img == c] = img[img == c] outs.append(colimg) if crop: return _composite_imagelist_to_imagelist(outs, _crop_nonzero) else: return outs Yes writing in C++ would make the code run much faster, however, the development time would be much higher and at this point, I was just trying to get an understanding of what kind of DSL is needed. As you can see the top solution, he truly put an insane amount of engineering effort into the competition, hats off. Training a classifier on ‚Äúconcepts‚Äù: At this point I‚Äôm thinking, okay so I can‚Äôt write such a huge DSL right now, and Chollet claims deep learning can‚Äôt do anything here. But its still the most powerful method I know of, so how can I try and leverage it, if at all. Can I prune my tree search with it somehow? What could I train a DL model on ARC for? If DL can do anything well, I thought, its probably going to be some vaguely specified concepts where coming up with rules is not straightforward. So I came up with a list of concepts which I annotated the train tasks for. These are quite similar to the priors mentioned in the paper but somewhat more specific. The idea was to train a classifier on these tasks, which can then help to prune the DFS. Background Color Object Splitting Rotational Symmetry Translational Symmetry Counting Drawing Lines Movement Multi Symbol Color Structure Size Change Repeating Swap Colors Being Inside of At first, I didn‚Äôt expect the model to perform too well, but with some augmentations (color swapping and rotations), surprisingly the model actually got a good accuracy on most of the concepts. Of course, this was just on multiple folds being split and the dataset is tiny, so I can‚Äôt say with confidence how good it actually was. The predictions on the test set seemed reasonable. Sadly, this was the middle of the pandemic, right near the beginning, and I wasn‚Äôt able to test this idea further and had to stop working on the competition midway due to factors beyond my control. Distributed ARC The main problem with the ARC to stay as a long term benchmark seems to be that the tasks are limited. It‚Äôs easy to overfit to the evaluation tasks and call the benchmark solved. Even if you don‚Äôt see the evaluation tasks, a 100 is not that many. But as Chollet had pointed out, its difficult for one person to come up with these tasks by hand. So how can we make ARC a long term benchmark? We use the power of crowd sourcing! I had this vague idea towards the end of 2020 but never tried to concretely formulate it. Then in 2021, Chollet discussed his idea of crowdsourcing at a high level in his interview at Machine Learning Street Talk. Not much has happened about it though, or none that I know of. I‚Äôve tried to outline my thoughts on what the benchmark can grow into using crowdsourcing. I call it Distributed ARC. Here‚Äôs my wishlist for a crowdsourced ARC benchmark The collection of tasks should be large (\u003e 10,000). To avoid overfitting. Anyone should be able to submit new tasks, versioning can be done each year to","date":"2022-02-07","objectID":"/thoughts-on-the-arc-benchmark/:0:6","tags":["Deep Learning","Benchmarks"],"title":"Thoughts on the ARC Benchmark","uri":"/thoughts-on-the-arc-benchmark/"}]